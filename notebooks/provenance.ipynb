{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlcbakery import bakery_client\n",
    "import pathlib\n",
    "client = bakery_client.Client(\"http://localhost:8000\", token=\"authtoken\")\n",
    "\n",
    "def add_rand_column_to_dataset(path_to_csv, column_name):\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "    df[column_name] = np.random.randint(0, 100)\n",
    "    df.to_csv(path_to_csv, index=False)\n",
    "\n",
    "def load_json(path):\n",
    "    # load json:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    raise Exception(\"Failed to load json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BakeryCollection(id=1, name='test25', description='', storage_info={'type': 'service_account', 'project_id': 'bakerydev', 'private_key_id': '39a66cf95640b85d8fdea6e424b21ae1bfdea6c8', 'private_key': '-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQD2b+Z+gJzJhHFm\\nlT5k7Gi9IPjr0bEpSJYrk8d6/aJJmWHArwQw/DGyXLd+TWmkuQyAZbZc6iCjQQWK\\neX26uir6SisrQBQ6/exgXFolXOHE6TRPWq/O/Ic/WHTW25wci4WS60FM5mpCj5oZ\\nDM3P4Tj7Yg0wQyaS2rY3LJjUxiG56mZEbfsGiyYVD6WOthAHxRS/wihbQkXzwy4P\\nq1HMalhF049OqyMxdmdWCmMwRsyV1kdKGSRDTHG+nWwrg+/4ZFllTlV8hFZ9E5ED\\nYv2CK5xIkXRJCECneRIbKRaXfSz9O9GN6W0UZ17wUoaX47ydquMYYQyqkaZIhkVY\\n8ktkTPK/AgMBAAECgf9CLVCf3L0I7rSx13p0CUkraE3Jb33u0QkhDCiZpaMv//Qz\\n9rt2nNUEw3d+6SJErA89mEN86OiFeLOOX5OxBuewVtz/0IqyibaSxEIz9xrCAwqP\\nKEkqTFTAUIre7t1q5VHAd61mzHlS1DELZHklXdPCwiQuGs4MJ7N5r9FzGQbAsNs0\\ni1LE57vo1q/I7l94SkMi2mjsU/00qvebuKXamaG9DAyCUP/9skfvtWyO8P/z3znr\\n6sCT442Pb7SR9Wagjz4j+QTbNwOrUuAsXwMNRLHb1SBhuMllMk/6gOyoo2XI15YG\\nOEkcVM3L+iSVJsT30zHtjQRzakYPO2wgrllfFGkCgYEA+8nNh+kHRxd4eBpJJkNI\\n+mhU8x/hAqly6HPe5zli28NFSaTurpghUy15tRK72+GCX+/SfbOtSuEQzrqF2lRq\\nnBTkwV+tUrOlo458BdqO81h59tlS4G3PEi8CdopS02AzCXh1f5mO2BWgiIcMQfNp\\nv9nGtQyP9Y31pTLL9BJaxoUCgYEA+o8u0y6iF+SVQMRJNwHLTR/oTtTbPEc3khJo\\nVkk5hB2Nd9gixMmw3dhzu/UNQ+kOq+PC7gOCxBO7kB4h1b/nbWSX+eJlpy5emVOI\\nS27FB3PVDDazkQbVeBa+e3MZ8cfhzvcGfIiRNIMGY02xDEcaSXZXz+Ww2cGMOvl9\\n/0oGQXMCgYEAvgGbyGRhyZQ8pOWxLd64w77X3GDCPHAC9sf5/iOgJpdFQj3koHzN\\nKSe4IJzyhwu2hw9DmPhuXYNaW6cGO6xYh1B9EDBTa4WGCdcMYJ5+IcUyxMVbdWFo\\nUiCHi0z3E+wUl6D/ijsnQImRi3XeGibGNrDw8s94E6X9KZ0brCPtvMECgYEAhIbH\\nVZwM0GZvK1qm/xlL1vdRgKDFdRSWKATkev8wFNkswn7npP6pNi4OyRPMeGcFA5Xs\\nvMfOmrd1Nb06WDywxAO4/sroyYyVLLQqGvvz6wUxYsxsHgFnV6VwCwvOZTw8C6RN\\nIgF6DDRhZJTrWHsjREf8GlJ7QDJHYcolAqq8tFECgYAehFx1VEN4lT+JDtcDpzfo\\nR9qTxU8vcrNRjM0DT55b25ypgUXXJeFIl02rgFpMF2PyToozdUPHLbGRKayeBkO9\\nAnJFKLwffFYCagz8BQgu7twGzOMR2dEz7d+S0NUbk+zBX24RjBe3F62uxGcb6ZYH\\nU8ctCuljtUWomqfkKc1FvA==\\n-----END PRIVATE KEY-----\\n', 'client_email': 'jetty-bakery-storage-sa@bakerydev.iam.gserviceaccount.com', 'client_id': '112059975217540314476', 'auth_uri': 'https://accounts.google.com/o/oauth2/auth', 'token_uri': 'https://oauth2.googleapis.com/token', 'auth_provider_x509_cert_url': 'https://www.googleapis.com/oauth2/v1/certs', 'client_x509_cert_url': 'https://www.googleapis.com/robot/v1/metadata/x509/jetty-bakery-storage-sa%40bakerydev.iam.gserviceaccount.com', 'universe_domain': 'googleapis.com', 'bucket': 'jetty-bakery-storage'}, storage_provider='gcp')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client.find_or_create_by_collection_name(\"test25\")\n",
    "client.update_collection_storage_info(\"test25\", storage_info=load_json(\"../../bakerydev-bakery-storage-sa.json\"), storage_provider=\"gcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 20:38:21,054 - mlcbakery.bakery_client - INFO - Created .manifest.json in './datasets/C1_000'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'properties': {'type': 'dataset',\n",
       "  'name': 'C1_000',\n",
       "  'collection_name': 'test25',\n",
       "  'origin': 'jetty.io',\n",
       "  'metadata_version': '1.0.0'},\n",
       " 'parents': [{'generated_by': None}],\n",
       " 'assets': {'long_description': 'README.md', 'metadata': 'metadata.json'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepares the dataset folder for the bakery by creating a .manifest.json file based on the folder structure\n",
    "client.prepare_dataset(\"./datasets/C1_000\", params={\n",
    "    \"properties\": {\n",
    "        \"type\": \"dataset\",\n",
    "        \"name\": \"C1_000\",\n",
    "        \"collection_name\": \"test25\",\n",
    "        \"origin\": \"jetty.io\",\n",
    "        \"metadata_version\": \"1.0.0\",\n",
    "    },\n",
    "    \"parents\": [\n",
    "        {\n",
    "            \"generated_by\": None,\n",
    "        }\n",
    "    ],\n",
    "    \"assets\": {\n",
    "        \"long_description\": \"README.md\",\n",
    "        \"metadata\": \"metadata.json\"\n",
    "    }    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 20:38:36,917 - absl - WARNING - WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'jsonPath', '@language', 'fileObject', 'includes', 'regex', 'key', 'subField', 'fileProperty', 'transform', 'parentField', 'rai', 'cr', 'citeAs', 'format', 'path', 'replace', 'md5', 'conformsTo', 'dct', 'data', '@vocab', 'isLiveDataset', 'references', 'repeated', 'separator', 'examples', 'fileSet'}\n",
      "2025-05-12 20:38:36,933 - mlcbakery.bakery_client - INFO - Created tar.gz of data folder at /tmp/tmpombg1s0h.tar.gz\n",
      "2025-05-12 20:38:36,935 - mlcbakery.bakery_client - INFO - Pushing dataset 'test25/C1_004' to Bakery API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 20:38:36,988 - absl - WARNING - WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'jsonPath', '@language', 'fileObject', 'includes', 'regex', 'key', 'subField', 'fileProperty', 'transform', 'parentField', 'rai', 'cr', 'citeAs', 'format', 'path', 'replace', 'md5', 'conformsTo', 'dct', 'data', '@vocab', 'isLiveDataset', 'references', 'repeated', 'separator', 'examples', 'fileSet'}\n",
      "2025-05-12 20:38:37,014 - mlcbakery.bakery_client - ERROR - Request failed: 404 Client Error: Not Found for url: http://localhost:8000/api/v1/datasets/test25/C1_004/preview\n",
      "2025-05-12 20:38:37,016 - mlcbakery.bakery_client - INFO - Preview for dataset test25/C1_004 not found.\n",
      "2025-05-12 20:38:37,019 - mlcbakery.bakery_client - INFO - Updating dataset C1_004 in collection test25\n",
      "2025-05-12 20:38:37,065 - mlcbakery.bakery_client - INFO - Uploading data file for dataset C1_004 in collection test25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination 'datasets/C1_004' already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 20:38:37,388 - absl - WARNING - WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'jsonPath', '@language', 'fileObject', 'includes', 'regex', 'key', 'subField', 'fileProperty', 'transform', 'parentField', 'rai', 'cr', 'citeAs', 'format', 'path', 'replace', 'md5', 'conformsTo', 'dct', 'data', '@vocab', 'isLiveDataset', 'references', 'repeated', 'separator', 'examples', 'fileSet'}\n",
      "2025-05-12 20:38:37,410 - mlcbakery.bakery_client - ERROR - Request failed: 404 Client Error: Not Found for url: http://localhost:8000/api/v1/datasets/test25/C1_004/preview\n",
      "2025-05-12 20:38:37,411 - mlcbakery.bakery_client - INFO - Preview for dataset test25/C1_004 not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BakeryDataset(id=4, name='C1_004', collection_id=1, parent_collection_dataset=None, metadata=Dataset(jsonld={'@context': {'sc': 'https://schema.org/', 'name': 'sc:name', 'description': 'sc:description', 'license': 'sc:license', 'distribution': 'sc:distribution', 'recordSet': 'sc:recordSet', 'field': 'sc:field', 'dataType': 'sc:dataType', 'source': 'sc:source', 'extract': 'sc:extract', 'column': 'sc:column', 'contentUrl': 'sc:contentUrl', 'encodingFormat': 'sc:encodingFormat', 'citation': 'sc:citation', 'datePublished': 'sc:datePublished', 'version': 'sc:version', '@base': 'cr_base_iri/'}, '@type': 'sc:Dataset', 'name': 'A1_000', 'description': 'Country population and favourite food dataset.', 'license': 'CC-BY-4.0', 'citation': 'Source: Example dataset for Jetty/MLC Bakery demo.', 'datePublished': '2024-06-07', 'version': '1.0.0', 'distribution': [{'@type': 'https://schema.org/FileObject', 'name': 'data.csv', 'contentUrl': 'data/data.csv', 'encodingFormat': 'text/csv', 'https://schema.org/sha256': 'f7968a7a31a9cd1cbe5b2d2306c574cfd5934c8166f5e158669440baf72237d5'}], 'recordSet': [{'name': 'countries', 'distribution': 'data.csv', 'field': [{'name': 'Country name', 'description': 'Name of the country.', 'dataType': 'sc:Text', 'source': {'extract': {'column': 'Country name'}}}, {'name': 'Population', 'description': 'Population of the country (as string, may contain commas).', 'dataType': 'sc:Text', 'source': {'extract': {'column': 'Population'}}}, {'name': 'Favourite food', 'description': 'Most popular food in the country.', 'dataType': 'sc:Text', 'source': {'extract': {'column': 'Favourite food'}}}]}]}, operations=OperationGraph(issues=Issues(_errors=set(), _warnings=set()), operations=<mlcroissant._src.operation_graph.base_operation.Operations object at 0x79d2d044d280>), metadata=Metadata(uuid=\"A1_000\"), debug=False, mapping=None), preview=None, format='csv', created_at='2025-05-12T20:37:51.296850Z', metadata_version='1.0.0', data_path='datasets/C1_004/data', long_description='# Dataset: A1_000\\n\\n**Description:**  \\nCountry population and favourite food dataset.\\n\\n**License:**  \\nCC-BY-4.0\\n\\n**Citation:**  \\nSource: Example dataset for Jetty/MLC Bakery demo.\\n\\n**Date Published:**  \\n2024-06-07\\n\\n**Version:**  \\n1.0.0\\n\\n---\\n\\n## Data Files\\n\\n- **data.csv**  \\n  - Format: text/csv  \\n  - Path: `data/data.csv`  \\n  - SHA256: `f7968a7a31a9cd1cbe5b2d2306c574cfd5934c8166f5e158669440baf72237d5`\\n\\n---\\n\\n## Record Set: countries\\n\\n| Field Name      | Description                                   | Data Type |\\n|-----------------|-----------------------------------------------|-----------|\\n| Country name    | Name of the country.                          | Text      |\\n| Population      | Population of the country (as string, may contain commas). | Text      |\\n| Favourite food  | Most popular food in the country.             | Text      |\\n\\n---\\n\\n## Source\\n\\nThis dataset is provided as an example for Jetty/MLC Bakery demo. ', asset_origin='jetty.io')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = \"C1_000\"\n",
    "target = \"C1_004\"\n",
    "try:\n",
    "    client.duplicate_dataset(f\"datasets/{source}\", f\"datasets/{target}\", params={\n",
    "        \"properties\": {\n",
    "            \"name\": target\n",
    "        },\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "add_rand_column_to_dataset(f\"datasets/{target}/data/data.csv\", target)\n",
    "client.save_to_bakery(f\"./datasets/{target}\", upload_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5,\n",
       " 'name': 'C1_003',\n",
       " 'collection_name': 'test25',\n",
       " 'entity_type': 'dataset',\n",
       " 'activity_id': 9,\n",
       " 'activity_name': 'created',\n",
       " 'children': []}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_upstream_entities(\"test25\", \"C1_003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from sqlalchemy.future import select\n",
    "from sqlalchemy.orm import selectinload\n",
    "from mlcbakery.models import Entity, Collection, Activity  # Assuming these are your models\n",
    "from mlcbakery.database import get_async_db  # Assuming this is your DB session provider\n",
    "from contextlib import aclosing # Needed for robust generator closing\n",
    "\n",
    "# async def fetch_entity(entity_id: int):\n",
    "#     db_generator = get_async_db()  # Get the generator object\n",
    "#     async with aclosing(db_generator) as agen: # Ensure the generator is closed properly\n",
    "#         session = await anext(agen)  # Get the first yielded value (the session)\n",
    "        \n",
    "#         # Now use the session as before\n",
    "#         stmt = (\n",
    "#             select(Entity)\n",
    "#             .options(\n",
    "#                 selectinload(Entity.collection),\n",
    "#                 selectinload(Entity.input_activities).selectinload(\n",
    "#                     Activity.input_entities\n",
    "#                 ).selectinload(Entity.collection),\n",
    "#                 selectinload(Entity.input_activities).selectinload(Activity.input_entities),\n",
    "#                 selectinload(Entity.output_activities).selectinload(Activity.input_entities),\n",
    "#             )\n",
    "#             .where(Entity.id == entity_id)\n",
    "#         )\n",
    "#         result = await session.execute(stmt) \n",
    "#         entity = result.scalar_one_or_none()\n",
    "        \n",
    "#         return entity\n",
    "async def fetch_entity(entity_id: int):\n",
    "    \"\"\"Fetches a single entity with preloaded relationships for provenance tracing.\"\"\"\n",
    "    db_generator = get_async_db()\n",
    "    async with aclosing(db_generator) as agen:\n",
    "        session = await anext(agen)\n",
    "        stmt = (\n",
    "            select(Entity)\n",
    "            .options(\n",
    "                selectinload(Entity.collection),\n",
    "                selectinload(Entity.input_activities).selectinload(\n",
    "                    Activity.input_entities\n",
    "                ).selectinload(Entity.collection),\n",
    "                 selectinload(Entity.input_activities).selectinload(Activity.input_entities),\n",
    "                selectinload(Entity.output_activities).selectinload(Activity.input_entities),\n",
    "            )\n",
    "            .where(Entity.id == entity_id)\n",
    "        )\n",
    "        result = await session.execute(stmt)\n",
    "        entity = result.scalar_one_or_none()\n",
    "        return entity\n",
    "\n",
    "\n",
    "async def get_provenance_path(entity_id: int, visited_ids: set[int] | None = None) -> list:\n",
    "    \"\"\"\n",
    "    Recursively fetches the provenance path from the given entity_id upwards,\n",
    "    including cycle detection.\n",
    "\n",
    "    Args:\n",
    "        entity_id: The ID of the starting (childmost) entity.\n",
    "        visited_ids: A set of entity IDs already visited in the current path\n",
    "                     to detect cycles. Should be None for the initial call.\n",
    "\n",
    "    Returns:\n",
    "        A list representing the provenance path:\n",
    "        [ChildEntity, ChildActivity, ParentEntity, ParentActivity, ..., RootEntity, RootActivity]\n",
    "        Returns an empty list if the entity is not found or a cycle is detected.\n",
    "        Returns [Entity] if the entity has no creation activity linking to a parent.\n",
    "    \"\"\"\n",
    "    # Initialize visited_ids set for the first call\n",
    "    if visited_ids is None:\n",
    "        visited_ids = set()\n",
    "\n",
    "    # --- Cycle Detection ---\n",
    "    if entity_id in visited_ids:\n",
    "        print(f\"Cycle detected! Entity ID {entity_id} already visited in this path. Stopping recursion.\")\n",
    "        return [] # Return empty list or raise an exception/specific indicator\n",
    "\n",
    "    print(f\"Fetching path starting from entity ID: {entity_id}\") # Debug print\n",
    "\n",
    "    # Add current entity ID to the visited set for this recursive path\n",
    "    current_visited_ids = visited_ids | {entity_id} # Use union operator to create a new set\n",
    "\n",
    "    entity = await fetch_entity(entity_id)\n",
    "    if not entity:\n",
    "        print(f\"Entity with ID {entity_id} not found.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Found entity: {entity.name} (ID: {entity.id})\") # Debug print\n",
    "\n",
    "    if not entity.input_activities:\n",
    "        print(f\"Entity '{entity.name}' (ID: {entity.id}) has no input activities. Treating as root.\")\n",
    "        return [entity]\n",
    "\n",
    "    creation_activity = entity.input_activities[0]\n",
    "    print(f\"Using creation activity: {creation_activity.name} (ID: {creation_activity.id})\") # Debug print\n",
    "\n",
    "    if not creation_activity.input_entities:\n",
    "        print(f\"Activity '{creation_activity.name}' (ID: {creation_activity.id}) has no input entities. Reached root.\")\n",
    "        return [entity, creation_activity]\n",
    "\n",
    "    parent_entity_stub = creation_activity.input_entities[0]\n",
    "    print(f\"Found parent entity stub: {parent_entity_stub.name} (ID: {parent_entity_stub.id})\") # Debug print\n",
    "\n",
    "    # --- Recursive Call with updated visited set ---\n",
    "    path_from_parent = await get_provenance_path(parent_entity_stub.id, current_visited_ids)\n",
    "\n",
    "    # Only prepend if the recursive call didn't hit a cycle or dead end\n",
    "    if not path_from_parent and parent_entity_stub.id in current_visited_ids:\n",
    "         # If path_from_parent is empty specifically because *this* parent call detected a cycle\n",
    "         # involving the parent_entity_stub, we don't prepend the current segment.\n",
    "         # We only return [] to propagate the cycle signal up.\n",
    "         print(f\"Not prepending segment for {entity.name} as parent call detected cycle.\")\n",
    "         return []\n",
    "    elif not path_from_parent and not await fetch_entity(parent_entity_stub.id):\n",
    "        # If path_from_parent is empty because the parent entity wasn't found\n",
    "        print(f\"Not prepending segment for {entity.name} as parent entity {parent_entity_stub.id} was not found.\")\n",
    "        # Depending on desired behavior, you might still prepend [entity, creation_activity]\n",
    "        # return [entity, creation_activity] # Or return [] to indicate failure\n",
    "        return []\n",
    "\n",
    "\n",
    "    current_path_segment = [entity, creation_activity]\n",
    "    print(f\"Returning path segment for {entity.name}: {[item.name if hasattr(item, 'name') else 'Unknown' for item in current_path_segment]}\") # Debug print\n",
    "    final_path = current_path_segment + path_from_parent\n",
    "    print(f\"Returning combined path: {[item.name if hasattr(item, 'name') else 'Unknown' for item in final_path]}\") # Debug print\n",
    "    return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 02:14:04,409 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2025-05-13 02:14:04,411 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2025-05-13 02:14:04,416 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2025-05-13 02:14:04,418 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2025-05-13 02:14:04,422 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2025-05-13 02:14:04,422 INFO sqlalchemy.engine.Engine [raw sql] ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 02:14:04,427 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-05-13 02:14:04,431 INFO sqlalchemy.engine.Engine SELECT entities.id, entities.name, entities.entity_type, entities.created_at, entities.asset_origin, entities.collection_id \n",
      "FROM entities \n",
      "WHERE entities.id = $1::INTEGER\n",
      "2025-05-13 02:14:04,432 INFO sqlalchemy.engine.Engine [generated in 0.00099s] (1,)\n",
      "2025-05-13 02:14:04,438 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,438 INFO sqlalchemy.engine.Engine [generated in 0.00078s] (1,)\n",
      "2025-05-13 02:14:04,446 INFO sqlalchemy.engine.Engine SELECT entities_1.id AS entities_1_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at, activities.output_entity_id AS activities_output_entity_id \n",
      "FROM entities AS entities_1 JOIN activity_entities AS activity_entities_1 ON entities_1.id = activity_entities_1.entity_id JOIN activities ON activities.id = activity_entities_1.activity_id \n",
      "WHERE entities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,446 INFO sqlalchemy.engine.Engine [generated in 0.00104s] (1,)\n",
      "2025-05-13 02:14:04,452 INFO sqlalchemy.engine.Engine SELECT activities.output_entity_id AS activities_output_entity_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at \n",
      "FROM activities \n",
      "WHERE activities.output_entity_id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,452 INFO sqlalchemy.engine.Engine [generated in 0.00074s] (1,)\n",
      "2025-05-13 02:14:04,457 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,458 INFO sqlalchemy.engine.Engine [generated in 0.00096s] (7,)\n",
      "2025-05-13 02:14:04,463 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,463 INFO sqlalchemy.engine.Engine [generated in 0.00068s] (1,)\n",
      "2025-05-13 02:14:04,466 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,467 INFO sqlalchemy.engine.Engine [generated in 0.00063s] (1,)\n",
      "2025-05-13 02:14:04,469 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "2025-05-13 02:14:04,471 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-05-13 02:14:04,472 INFO sqlalchemy.engine.Engine SELECT entities.id, entities.name, entities.entity_type, entities.created_at, entities.asset_origin, entities.collection_id \n",
      "FROM entities \n",
      "WHERE entities.id = $1::INTEGER\n",
      "2025-05-13 02:14:04,473 INFO sqlalchemy.engine.Engine [cached since 0.04225s ago] (2,)\n",
      "2025-05-13 02:14:04,476 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,476 INFO sqlalchemy.engine.Engine [cached since 0.03867s ago] (1,)\n",
      "2025-05-13 02:14:04,479 INFO sqlalchemy.engine.Engine SELECT entities_1.id AS entities_1_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at, activities.output_entity_id AS activities_output_entity_id \n",
      "FROM entities AS entities_1 JOIN activity_entities AS activity_entities_1 ON entities_1.id = activity_entities_1.entity_id JOIN activities ON activities.id = activity_entities_1.activity_id \n",
      "WHERE entities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,479 INFO sqlalchemy.engine.Engine [cached since 0.03365s ago] (2,)\n",
      "2025-05-13 02:14:04,481 INFO sqlalchemy.engine.Engine SELECT activities.output_entity_id AS activities_output_entity_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at \n",
      "FROM activities \n",
      "WHERE activities.output_entity_id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,482 INFO sqlalchemy.engine.Engine [cached since 0.03011s ago] (2,)\n",
      "2025-05-13 02:14:04,484 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,485 INFO sqlalchemy.engine.Engine [cached since 0.02772s ago] (8,)\n",
      "2025-05-13 02:14:04,488 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER, $2::INTEGER)\n",
      "2025-05-13 02:14:04,491 INFO sqlalchemy.engine.Engine [cached since 0.0286s ago] (2, 7)\n",
      "2025-05-13 02:14:04,497 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:14:04,498 INFO sqlalchemy.engine.Engine [cached since 0.03175s ago] (1,)\n",
      "2025-05-13 02:14:04,500 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C1_000', 'generated', 'C1_001')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downstream entities:\n",
    "entity_1 = await fetch_entity(1)\n",
    "entity_2_id = entity_1.input_activities[0].output_entity_id\n",
    "entity_2 = await fetch_entity(entity_2_id)\n",
    "entity_1.name, entity_1.input_activities[0].name, entity_2.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 02:17:44,806 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-05-13 02:17:44,807 INFO sqlalchemy.engine.Engine SELECT entities.id, entities.name, entities.entity_type, entities.created_at, entities.asset_origin, entities.collection_id \n",
      "FROM entities \n",
      "WHERE entities.id = $1::INTEGER\n",
      "2025-05-13 02:17:44,808 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (2,)\n",
      "2025-05-13 02:17:44,811 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,812 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (1,)\n",
      "2025-05-13 02:17:44,814 INFO sqlalchemy.engine.Engine SELECT entities_1.id AS entities_1_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at, activities.output_entity_id AS activities_output_entity_id \n",
      "FROM entities AS entities_1 JOIN activity_entities AS activity_entities_1 ON entities_1.id = activity_entities_1.entity_id JOIN activities ON activities.id = activity_entities_1.activity_id \n",
      "WHERE entities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,815 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (2,)\n",
      "2025-05-13 02:17:44,817 INFO sqlalchemy.engine.Engine SELECT activities.output_entity_id AS activities_output_entity_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at \n",
      "FROM activities \n",
      "WHERE activities.output_entity_id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,818 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (2,)\n",
      "2025-05-13 02:17:44,820 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,820 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (8,)\n",
      "2025-05-13 02:17:44,823 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER, $2::INTEGER)\n",
      "2025-05-13 02:17:44,823 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (2, 7)\n",
      "2025-05-13 02:17:44,826 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,827 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (1,)\n",
      "2025-05-13 02:17:44,829 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "2025-05-13 02:17:44,831 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-05-13 02:17:44,832 INFO sqlalchemy.engine.Engine SELECT entities.id, entities.name, entities.entity_type, entities.created_at, entities.asset_origin, entities.collection_id \n",
      "FROM entities \n",
      "WHERE entities.id = $1::INTEGER\n",
      "2025-05-13 02:17:44,833 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (1,)\n",
      "2025-05-13 02:17:44,835 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,836 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (1,)\n",
      "2025-05-13 02:17:44,838 INFO sqlalchemy.engine.Engine SELECT entities_1.id AS entities_1_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at, activities.output_entity_id AS activities_output_entity_id \n",
      "FROM entities AS entities_1 JOIN activity_entities AS activity_entities_1 ON entities_1.id = activity_entities_1.entity_id JOIN activities ON activities.id = activity_entities_1.activity_id \n",
      "WHERE entities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,839 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (1,)\n",
      "2025-05-13 02:17:44,841 INFO sqlalchemy.engine.Engine SELECT activities.output_entity_id AS activities_output_entity_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at \n",
      "FROM activities \n",
      "WHERE activities.output_entity_id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,841 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (1,)\n",
      "2025-05-13 02:17:44,843 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,844 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (7,)\n",
      "2025-05-13 02:17:44,847 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,848 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (1,)\n",
      "2025-05-13 02:17:44,851 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:17:44,853 INFO sqlalchemy.engine.Engine [cached since 220.4s ago] (1,)\n",
      "2025-05-13 02:17:44,854 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('created',)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_2 = await fetch_entity(2)\n",
    "trajectory = []\n",
    "trajectory.append((entity_2.output_activities[0].name, entity_2.name))\n",
    "trajectory.append((entity_2.output_activities[1].input_entities[0].name, entity_2.output_activities[1].name, entity_2.name))\n",
    "trajectory\n",
    "\n",
    "\n",
    "trajectory = []\n",
    "entity = await fetch_entity(1)\n",
    "while entity:\n",
    "\n",
    "    if entity.output_activities:\n",
    "        for output_activity in entity.output_activities:\n",
    "            if output_activity.input_entities:\n",
    "                entity = await fetch_entity(output_activity.input_entities[0].name)\n",
    "            else:\n",
    "                entity = None\n",
    "        \n",
    "            if entity:\n",
    "                trajectory.append((output_activity.name, entity.name))\n",
    "            else:\n",
    "                trajectory.append((output_activity.name,))\n",
    "\n",
    "    else:\n",
    "        trajectory.append((entity.name,))\n",
    "        entity = None\n",
    "trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 02:09:33,562 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-05-13 02:09:33,565 INFO sqlalchemy.engine.Engine SELECT entities.id, entities.name, entities.entity_type, entities.created_at, entities.asset_origin, entities.collection_id \n",
      "FROM entities \n",
      "WHERE entities.id = $1::INTEGER\n",
      "2025-05-13 02:09:33,566 INFO sqlalchemy.engine.Engine [cached since 1829s ago] (2,)\n",
      "2025-05-13 02:09:33,570 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:09:33,571 INFO sqlalchemy.engine.Engine [cached since 1829s ago] (1,)\n",
      "2025-05-13 02:09:33,574 INFO sqlalchemy.engine.Engine SELECT entities_1.id AS entities_1_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at, activities.output_entity_id AS activities_output_entity_id \n",
      "FROM entities AS entities_1 JOIN activity_entities AS activity_entities_1 ON entities_1.id = activity_entities_1.entity_id JOIN activities ON activities.id = activity_entities_1.activity_id \n",
      "WHERE entities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:09:33,575 INFO sqlalchemy.engine.Engine [cached since 1829s ago] (2,)\n",
      "2025-05-13 02:09:33,577 INFO sqlalchemy.engine.Engine SELECT activities.output_entity_id AS activities_output_entity_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at \n",
      "FROM activities \n",
      "WHERE activities.output_entity_id IN ($1::INTEGER)\n",
      "2025-05-13 02:09:33,578 INFO sqlalchemy.engine.Engine [cached since 1829s ago] (2,)\n",
      "2025-05-13 02:09:33,581 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 02:09:33,581 INFO sqlalchemy.engine.Engine [cached since 1829s ago] (8,)\n",
      "2025-05-13 02:09:33,584 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER, $2::INTEGER)\n",
      "2025-05-13 02:09:33,584 INFO sqlalchemy.engine.Engine [cached since 1829s ago] (2, 7)\n",
      "2025-05-13 02:09:33,587 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 02:09:33,587 INFO sqlalchemy.engine.Engine [cached since 1829s ago] (1,)\n",
      "2025-05-13 02:09:33,589 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C1_001', 'generated', 'C1_000')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upstream entities:\n",
    "\n",
    "entity_2 = await fetch_entity(2)\n",
    "output_activity = entity_2.output_activities[1]\n",
    "entity_2.name, output_activity.name , output_activity.input_entities[0].name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching path starting from entity ID: 2\n",
      "2025-05-13 01:48:59,133 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-05-13 01:48:59,137 INFO sqlalchemy.engine.Engine SELECT entities.id, entities.name, entities.entity_type, entities.created_at, entities.asset_origin, entities.collection_id \n",
      "FROM entities \n",
      "WHERE entities.id = $1::INTEGER\n",
      "2025-05-13 01:48:59,140 INFO sqlalchemy.engine.Engine [cached since 594.1s ago] (2,)\n",
      "2025-05-13 01:48:59,145 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 01:48:59,149 INFO sqlalchemy.engine.Engine [cached since 594.1s ago] (1,)\n",
      "2025-05-13 01:48:59,152 INFO sqlalchemy.engine.Engine SELECT entities_1.id AS entities_1_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at, activities.output_entity_id AS activities_output_entity_id \n",
      "FROM entities AS entities_1 JOIN activity_entities AS activity_entities_1 ON entities_1.id = activity_entities_1.entity_id JOIN activities ON activities.id = activity_entities_1.activity_id \n",
      "WHERE entities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 01:48:59,155 INFO sqlalchemy.engine.Engine [cached since 594.1s ago] (2,)\n",
      "2025-05-13 01:48:59,158 INFO sqlalchemy.engine.Engine SELECT activities.output_entity_id AS activities_output_entity_id, activities.id AS activities_id, activities.name AS activities_name, activities.created_at AS activities_created_at \n",
      "FROM activities \n",
      "WHERE activities.output_entity_id IN ($1::INTEGER)\n",
      "2025-05-13 01:48:59,159 INFO sqlalchemy.engine.Engine [cached since 594.1s ago] (2,)\n",
      "2025-05-13 01:48:59,163 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER)\n",
      "2025-05-13 01:48:59,166 INFO sqlalchemy.engine.Engine [cached since 594.1s ago] (8,)\n",
      "2025-05-13 01:48:59,170 INFO sqlalchemy.engine.Engine SELECT activities_1.id AS activities_1_id, entities.id AS entities_id, entities.name AS entities_name, entities.entity_type AS entities_entity_type, entities.created_at AS entities_created_at, entities.asset_origin AS entities_asset_origin, entities.collection_id AS entities_collection_id \n",
      "FROM activities AS activities_1 JOIN activity_entities AS activity_entities_1 ON activities_1.id = activity_entities_1.activity_id JOIN entities ON entities.id = activity_entities_1.entity_id \n",
      "WHERE activities_1.id IN ($1::INTEGER, $2::INTEGER)\n",
      "2025-05-13 01:48:59,170 INFO sqlalchemy.engine.Engine [cached since 594.1s ago] (2, 7)\n",
      "2025-05-13 01:48:59,174 INFO sqlalchemy.engine.Engine SELECT collections.id AS collections_id, collections.name AS collections_name, collections.description AS collections_description, collections.storage_info AS collections_storage_info, collections.storage_provider AS collections_storage_provider \n",
      "FROM collections \n",
      "WHERE collections.id IN ($1::INTEGER)\n",
      "2025-05-13 01:48:59,177 INFO sqlalchemy.engine.Engine [cached since 594.1s ago] (1,)\n",
      "2025-05-13 01:48:59,180 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "Found entity: C1_001 (ID: 2)\n",
      "Using creation activity: generated (ID: 8)\n",
      "Found parent entity stub: C1_001 (ID: 2)\n",
      "Cycle detected! Entity ID 2 already visited in this path. Stopping recursion.\n",
      "Not prepending segment for C1_001 as parent call detected cycle.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_provenance_path(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.push_dataset(\"notebooks/datasets/A1_000\")\n",
    "\n",
    "# client.duplicate_dataset(\"notebooks/datasets/A1_000\", \"notebooks/datasets/B1_000\", params={\n",
    "#     \"properties\": {\n",
    "#         \"name\": \"B1_000\"\n",
    "#     },\n",
    "# }, attributed_to=attributed_to)\n",
    "# add_rand_column_to_dataset(\"notebooks/datasets/B1_000/data.csv\", \"B1\")\n",
    "# client.push_dataset(\"notebooks/datasets/B1_000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take dataset A_### \n",
    "# generate some metadata\n",
    "# save the metadata + dataset as A1_###\n",
    "\n",
    "# grab a dataset A1_###\n",
    "# change the dataset + metadata\n",
    "# save the metadata + dataset as B1_###\n",
    "\n",
    "# grab a dataset A1_###\n",
    "# change the dataset + metadata\n",
    "# save the metadata + dataset as C1_###\n",
    "\n",
    "\n",
    "# grab a dataset B1_###\n",
    "# change the dataset + metadata\n",
    "# save the metadata + dataset as D1_###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'rai', 'examples'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found the following 3 warning(s) during the validation:\n",
      "  -  [Metadata(RefRef)] Property \"http://mlcommons.org/croissant/citeAs\" is recommended, but does not exist.\n",
      "  -  [Metadata(RefRef)] Property \"https://schema.org/datePublished\" is recommended, but does not exist.\n",
      "  -  [Metadata(RefRef)] Property \"https://schema.org/version\" is recommended, but does not exist.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ValidationResult(passed=True, message='The dataset passes Croissant validation.', details=None, valid_json_data=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlcbakery import croissant_validation\n",
    "result = croissant_validation.validate_json(\"../test.json\")\n",
    "croissant_validation.validate_croissant(result.valid_json_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
